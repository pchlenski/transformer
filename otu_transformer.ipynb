{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for OTU tables\n",
    "> Let's use NeuroSEED embeddings for classification, for instance\n",
    "\n",
    "Written partially by ChatGPT: https://chat.openai.com/share/3c087924-3c9f-4d42-993f-69657a4afbfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Get torch to use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class OTUTransformerClassifier(nn.Module):\n",
    "    def __init__(self, nhead, nhid, nlayers, dropout=0.5, embedding_dim=128):\n",
    "        super(OTUTransformerClassifier, self).__init__()\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=nhead, dim_feedforward=nhid, dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layers, num_layers=nlayers\n",
    "        )\n",
    "\n",
    "        # Binary classification output layer\n",
    "        self.fc = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Transformer expects input of shape [seq_len, batch_size, embedding_dim].\n",
    "        transformer_output = self.transformer_encoder(X.permute(1, 0, 2))\n",
    "\n",
    "        # Use the last output in the sequence for each item in the batch for classification.\n",
    "        transformer_output = transformer_output[-1, :, :]\n",
    "\n",
    "        output = torch.sigmoid(self.fc(transformer_output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(15783, 128)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to get embeddings working?\n",
    "\n",
    "# nn.Embedding.from_pretrained() expects a tensor of shape [num_embeddings, embedding_dim] as input.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "embeddings_path = \"/home/phil/mixture_embeddings/data/processed/otu_embeddings/yatsunenko/cnn_hyperbolic_128_otu_embeddings.csv\"\n",
    "embeddings_df = pd.read_csv(embeddings_path, index_col=0)\n",
    "\n",
    "embed = nn.Embedding.from_pretrained(torch.tensor(embeddings_df.values), freeze=True)\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.3231e-03,  2.8251e-03, -6.2945e-03,  5.3259e-03, -2.5479e-03,\n",
       "         3.6997e-03,  2.0449e-03, -6.1082e-04, -2.6830e-03, -1.7612e-03,\n",
       "         1.8971e-04,  4.4580e-03,  9.2340e-04, -1.8812e-03, -2.9178e-03,\n",
       "         1.4681e-02, -4.3525e-03, -5.4849e-03, -3.7471e-03,  4.0354e-03,\n",
       "         6.3417e-03, -1.8050e-03, -2.4388e-03, -4.5531e-03, -2.9844e-04,\n",
       "         2.0589e-03,  7.0418e-04,  4.9733e-04, -1.4950e-03, -9.9802e-04,\n",
       "        -5.5532e-03, -2.6107e-03,  2.3284e-03, -3.1904e-03,  2.5297e-03,\n",
       "        -2.7868e-03, -5.4514e-03, -1.7141e-02, -3.2478e-04,  3.7553e-03,\n",
       "        -3.1140e-03, -1.1644e-03, -1.3271e-03, -7.3676e-03,  5.2530e-03,\n",
       "         1.8106e-03, -2.7933e-04,  2.8870e-03, -1.6990e-03,  1.1634e-03,\n",
       "         2.0539e-03,  5.3673e-03, -6.8478e-04, -6.0461e-03,  2.3066e-03,\n",
       "         8.5648e-04,  1.4091e-03,  1.8466e-03,  1.5592e-03, -5.6731e-03,\n",
       "        -7.9304e-04, -7.1623e-04,  1.0376e-02, -8.2456e-04, -1.3137e-03,\n",
       "         1.7177e-03, -2.8253e-04,  5.4399e-04,  3.9007e-03, -1.3784e-03,\n",
       "         8.4481e-04, -6.5154e-03,  2.8608e-03, -2.6198e-05,  1.1530e-03,\n",
       "         3.7641e-03,  9.6053e-03,  3.1009e-03, -2.8739e-02,  3.8363e-03,\n",
       "        -8.9132e-04,  1.7749e-03, -2.2695e-03,  2.5402e-03, -3.1802e-03,\n",
       "         8.8661e-02, -3.9256e-03,  2.4727e-03,  6.1177e-03,  1.0299e-03,\n",
       "        -2.0168e-03,  7.2278e-03, -1.0677e-03, -2.9424e-03, -6.9693e-03,\n",
       "        -3.1331e-03, -6.7984e-03,  4.2663e-04,  4.0349e-03, -1.1513e-03,\n",
       "        -3.2667e-03, -6.1604e-03, -2.1311e-03,  3.1542e-03, -5.0791e-03,\n",
       "        -1.2867e-03,  2.7726e-03,  3.2614e-03, -4.9086e-04,  9.9739e-04,\n",
       "         1.6397e-03, -1.9691e-03,  3.0190e-03,  2.4388e-03, -6.4237e-03,\n",
       "        -4.5634e-02, -1.1412e-03,  7.3533e-03,  4.1297e-03, -2.8839e-03,\n",
       "        -1.5591e-02,  1.3027e-03,  2.7603e-03, -2.7488e-03,  1.8125e-03,\n",
       "        -1.5406e-03, -1.8712e-03, -6.3033e-04], dtype=torch.float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embed_by_name(name, embeddings=embeddings_df):\n",
    "    try:\n",
    "        return embed(torch.tensor(embeddings.index.get_loc(name)))\n",
    "    except KeyError:\n",
    "        return embed(torch.tensor(embeddings.index.get_loc(int(name))))\n",
    "\n",
    "embed_by_name(179499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>298804</th>\n",
       "      <th>179499</th>\n",
       "      <th>114400</th>\n",
       "      <th>1099710</th>\n",
       "      <th>1078207</th>\n",
       "      <th>1016598</th>\n",
       "      <th>1010876</th>\n",
       "      <th>1000592</th>\n",
       "      <th>1000113</th>\n",
       "      <th>100198</th>\n",
       "      <th>...</th>\n",
       "      <th>998383</th>\n",
       "      <th>997777</th>\n",
       "      <th>99793</th>\n",
       "      <th>998719</th>\n",
       "      <th>998524</th>\n",
       "      <th>999784</th>\n",
       "      <th>998905</th>\n",
       "      <th>998869</th>\n",
       "      <th>99960</th>\n",
       "      <th>99981</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amz4adltF.418711</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USygt25.F.418747</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USygt27.M.418861</th>\n",
       "      <td>3</td>\n",
       "      <td>87</td>\n",
       "      <td>1757</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amz5eldF.418421</th>\n",
       "      <td>43</td>\n",
       "      <td>905</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>317</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USygt52.M.418736</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 15783 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  298804  179499  114400  1099710  1078207  1016598  1010876  \\\n",
       "Sample                                                                         \n",
       "Amz4adltF.418711       9       0     267        0        6        1        0   \n",
       "USygt25.F.418747       0       7       0        0      146        0       13   \n",
       "USygt27.M.418861       3      87    1757        0       77        0       26   \n",
       "Amz5eldF.418421       43     905     252        0      126      317       28   \n",
       "USygt52.M.418736       0      25       0        2       32        0       18   \n",
       "\n",
       "                  1000592  1000113  100198  ...  998383  997777  99793  \\\n",
       "Sample                                      ...                          \n",
       "Amz4adltF.418711        1        0       0  ...       0       1      0   \n",
       "USygt25.F.418747        0        0       8  ...       0       0      0   \n",
       "USygt27.M.418861        0        0       2  ...       0       0      0   \n",
       "Amz5eldF.418421         0        0       0  ...       0      47      0   \n",
       "USygt52.M.418736        0        0       0  ...       0       0      0   \n",
       "\n",
       "                  998719  998524  999784  998905  998869  99960  99981  \n",
       "Sample                                                                  \n",
       "Amz4adltF.418711       0       0       0       0       0      0      0  \n",
       "USygt25.F.418747       0       0       0       0       0      0      0  \n",
       "USygt27.M.418861       0       0       0       0       0      0      0  \n",
       "Amz5eldF.418421        0       0       0       0       0      0      0  \n",
       "USygt52.M.418736       0       0       0       0       0      0      0  \n",
       "\n",
       "[5 rows x 15783 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otu_table_path = \"/home/phil/mixture_embeddings/data/interim/knight/yatsunenko_data.csv\"\n",
    "otu_table = pd.read_csv(otu_table_path, index_col=0)\n",
    "\n",
    "otu_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_otu_vector(otu_table_row):\n",
    "    # # Drop zero indices\n",
    "    otu_table_row = otu_table_row[otu_table_row != 0]\n",
    "    names = otu_table_row.index\n",
    "\n",
    "    # Normalize otu_vector, return as tensor\n",
    "    count_vector = torch.tensor(otu_table_row, dtype=torch.float32)\n",
    "    count_vector = count_vector.flatten()\n",
    "    count_vector /= count_vector.sum()\n",
    "\n",
    "    # Get embeddings\n",
    "    embeddings = [embed_by_name(name) for name in names]\n",
    "    embeddings = torch.stack(embeddings)\n",
    "\n",
    "    return embeddings, count_vector\n",
    "\n",
    "otu_embeddings, count_vector = embed_otu_vector(otu_table.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Embed all rows of otu_table\n",
    "# otu_tensor = torch.tensor(otu_table.values).float() # (n_samples, n_features)\n",
    "# otu_tensor /= otu_tensor.sum(dim=1, keepdim=True) # Normalize by total counts in each sample\n",
    "# embed_tensor = torch.stack([embed_by_name(name) for name in otu_table.columns]).float() # (n_features, embedding_dim)\n",
    "# otu_tensor = otu_tensor.unsqueeze(-1) # (n_samples, n_features, 1)\n",
    "# X = otu_tensor * embed_tensor # (n_samples, n_features, embedding_dim)\n",
    "# # X = torch.einsum(\"ij,jk->ijk\", [otu_tensor, embed_tensor]) # (n_samples, n_features, embedding_dim)\n",
    "\n",
    "# The above made tensors that were too big, with sparse values. Instead, we can use variable-length sequences:\n",
    "X = []\n",
    "for sample in otu_table.index:\n",
    "    sample_embeddings, sample_counts = embed_otu_vector(otu_table.loc[sample, :])\n",
    "    X.append(sample_embeddings * sample_counts.unsqueeze(-1)) # Need to unsqueeze to make Hadamard product work\n",
    "# Pad\n",
    "X = nn.utils.rnn.pad_sequence(X, batch_first=True).float()\n",
    "\n",
    "# Get labels\n",
    "y = pd.read_csv(\"/home/phil/mixture_embeddings/data/interim/knight/yatsunenko_metadata.csv\", index_col=0)[\"Sex\"]\n",
    "y = y.loc[otu_table.index]\n",
    "labeler = {\"female\": 0, \"male\": 1, \"unknown\": -1}\n",
    "y = torch.tensor(y.map(labeler).values).float()\n",
    "\n",
    "# Drop samples without labels\n",
    "X = X[y != -1]\n",
    "y = y[y != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorDataset' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/phil/transformer/otu_transformer.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/transformer/otu_transformer.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/transformer/otu_transformer.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m criterion \u001b[39m=\u001b[39m criterion\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/transformer/otu_transformer.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/transformer/otu_transformer.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Beumaeus/home/phil/transformer/otu_transformer.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Example training loop\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# Assume otu_embeddings is a 2D tensor of shape [num_otus, embedding_dim].\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "model = OTUTransformerClassifier(nhead=4, nhid=64, nlayers=3, embedding_dim=128).float()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Get everything on the GPU\n",
    "model = model.to(\"cuda\")\n",
    "criterion = criterion.to(\"cuda\")\n",
    "X = X.to(\"cuda\")\n",
    "y = y.to(\"cuda\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    pbar = tqdm(dataloader)\n",
    "    for X_batch, y_batch in pbar:  # Assume dataloader yields count_matrix and labels\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output.flatten(), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_description(f\"Epoch {epoch} loss: {sum(losses) / len(losses):.6f}\")\n",
    "        pbar.refresh()\n",
    "    print(f\"Epoch {epoch} loss: {sum(losses) / len(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/238 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Evaluate model - predict in batches\n",
    "\n",
    "outputs = []\n",
    "for X_batch, y_batch in tqdm(dataloader):\n",
    "    output = model(X_batch)\n",
    "    outputs.append(output)\n",
    "\n",
    "outputs = torch.cat(outputs).flatten().numpy()\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
